# GloVe
# Global Vectors for Word Representation

State of the art in 2014 and now regarded as a classic. Here are a couple of my favorite resources on the pre-trained vector method:

An overview of word embeddings and their connection to distributional semantic models [<a href="http://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove" title="aylien.com" rel="nofollow">1</a></li>]

Global Vectors for Word Representation presented by: Richard Socher of Stanford University [<a href="https://youtu.be/ASn7ExxLZws" title="YouTube.com" rel="nofollow">2</a></li>] 

You can find my own detailed explanation in section 7 of this post [<a href="https://www.xtiandata.com/single-post/2018/10/26/Shallow-Deep-Natural-Language-Processing" title="xtiandata.com" rel="nofollow">3</a></li>] 


Reference for the implementation:

Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014 [<a href="https://nlp.stanford.edu/pubs/glove.pdf" title="nlp.stanford.edu" rel="nofollow">4</a></li>] 
